<!-- Generated by SpecStory -->

# Parameters and model path specifications for AutoModel (2025-05-06 02:20Z)

_**User**_

帮我分析AutoModel可以传入什么参数，每个参数的类型和作用
另外多个model能否分别指定model的路径？怎么指定？

---

_**Assistant**_

### `AutoModel` 可传入的参数分析

以下是 `AutoModel` 的构造函数和相关方法中支持的参数、类型及作用：

#### 1. **`model`**
   - **类型**: `str`
   - **作用**: 指定模型的名称或路径。可以是本地路径或远程模型名称（如 Hugging Face Hub 的模型名称）。
   - **示例**: 
     ```python
     model="path/to/local/model"
     model="huggingface/model-name"
     ```

#### 2. **`device`**
   - **类型**: `str`
   - **作用**: 指定运行设备。支持 `"cuda"`, `"cpu"`, `"mps"`（Mac M1/M2 GPU），或具体的 GPU 编号（如 `"cuda:0"`）。
   - **默认值**: `"cuda"`
   - **示例**:
     ```python
     device="cuda:0"
     device="cpu"
     ```

#### 3. **`vad_model`**
   - **类型**: `str`
   - **作用**: 指定用于语音活动检测（VAD）的模型路径或名称。
   - **示例**:
     ```python
     vad_model="path/to/vad/model"
     ```

#### 4. **`vad_kwargs`**
   - **类型**: `dict`
   - **作用**: 配置 VAD 模型的参数，例如最大切割时长。
   - **默认值**: `{"max_single_segment_time": 30000}`
   - **示例**:
     ```python
     vad_kwargs={"max_single_segment_time": 15000}
     ```

#### 5. **`punc_model`**
   - **类型**: `str`
   - **作用**: 指定用于标点符号预测的模型路径或名称。
   - **示例**:
     ```python
     punc_model="path/to/punctuation/model"
     ```

#### 6. **`punc_kwargs`**
   - **类型**: `dict`
   - **作用**: 配置标点符号模型的参数。
   - **默认值**: 空字典 `{}`

#### 7. **`spk_model`**
   - **类型**: `str`
   - **作用**: 指定用于说话人分离的模型路径或名称。
   - **示例**:
     ```python
     spk_model="path/to/speaker/model"
     ```

#### 8. **`spk_kwargs`**
   - **类型**: `dict`
   - **作用**: 配置说话人分离模型的参数。
   - **默认值**: 空字典 `{}`

#### 9. **`spk_mode`**
   - **类型**: `str`
   - **作用**: 指定说话人分离的模式。支持 `"default"`, `"vad_segment"`, `"punc_segment"`。
   - **默认值**: `"punc_segment"`

#### 10. **`hub`**
   - **类型**: `str`
   - **作用**: 指定模型下载的来源。支持 `"hf"`（Hugging Face）或 `"ms"`（ModelScope）。
   - **默认值**: `"hf"`

#### 11. **`disable_update`**
   - **类型**: `bool`
   - **作用**: 是否禁用版本检查。
   - **默认值**: `False`

#### 12. **`log_level`**
   - **类型**: `str`
   - **作用**: 设置日志级别。支持 `"INFO"`, `"DEBUG"`, `"ERROR"` 等。
   - **默认值**: `"INFO"`

#### 13. **`batch_size`**
   - **类型**: `int`
   - **作用**: 指定推理时的批量大小。
   - **默认值**: `1`

#### 14. **`fp16` / `bf16`**
   - **类型**: `bool`
   - **作用**: 是否启用半精度（FP16 或 BF16）推理。
   - **默认值**: `False`

#### 15. **`seed`**
   - **类型**: `int`
   - **作用**: 设置随机种子以确保结果可复现。
   - **默认值**: `0`

#### 16. **`frontend`**
   - **类型**: `str`
   - **作用**: 指定前端处理模块（如特征提取器）。
   - **默认值**: `None`

#### 17. **`frontend_conf`**
   - **类型**: `dict`
   - **作用**: 配置前端处理模块的参数。

#### 18. **`tokenizer`**
   - **类型**: `str` 或 `list`
   - **作用**: 指定分词器类型或路径。
   - **默认值**: `None`

#### 19. **`tokenizer_conf`**
   - **类型**: `dict`
   - **作用**: 配置分词器的参数。

#### 20. **`init_param`**
   - **类型**: `str`
   - **作用**: 指定预训练模型的路径，用于加载权重。
   - **示例**:
     ```python
     init_param="path/to/pretrained/weights"
     ```

---

### 多个模型分别指定路径的方法

`AutoModel` 支持同时加载多个模型（如 `vad_model`, `punc_model`, `spk_model`），并为每个模型单独指定路径。以下是具体方法：

#### 示例代码
```python
from funasr import AutoModel

model = AutoModel(
    model="path/to/asr/model",          # 主模型路径
    vad_model="path/to/vad/model",      # VAD 模型路径
    punc_model="path/to/punc/model",    # 标点模型路径
    spk_model="path/to/spk/model",      # 说话人分离模型路径
    device="cuda:0",                    # 指定设备
    vad_kwargs={"max_single_segment_time": 15000},  # VAD 参数
    punc_kwargs={},                     # 标点模型参数
    spk_kwargs={},                      # 说话人分离参数
    spk_mode="punc_segment",            # 说话人分离模式
)
```

#### 注意事项
1. **模型路径**: 每个模型的路径可以是本地路径或远程模型名称。
2. **参数配置**: 每个模型的参数可以通过对应的 `*_kwargs` 参数单独配置。
3. **设备共享**: 所有模型默认使用同一个设备（通过 `device` 参数指定）。

---

_**User**_

我传入的是
model = AutoModel(
    model=config["paraformer_path"],
    vad_model=config["vad_model_path"],
    punc_model=config["punc_model_path"],
    spk_model=config["spk_model_path"],
    disable_update=True,
)

为什么还会有model_conf的提示？
2025-05-06 11:05:03,148 - INFO - Starting ASR CLI processing...
2025-05-06 11:05:03,150 - INFO - config.yaml loaded successfully.
funasr version: 1.2.6.
2025-05-06 11:05:03,150 - INFO - model_conf is not provided in kwargs, will download from modelscope(ms)
2025-05-06 11:05:03,150 - INFO - download models from model hub: hf
2025-05-06 11:05:05,267 - INFO - Loading pretrained params from D:\Programming\Model_Store\paraformer-zh\model.pt
2025-05-06 11:05:05,278 - INFO - ckpt: D:\Programming\Model_Store\paraformer-zh\model.pt
2025-05-06 11:05:06,698 - INFO - scope_map: ['module.', 'None']
2025-05-06 11:05:06,698 - INFO - excludes: None

---

_**Assistant**_

从日志和代码来看，`model_conf` 的提示是因为在 `build_model` 方法中，如果没有显式提供 `model_conf` 参数，代码会尝试从模型仓库（如 ModelScope 或 Hugging Face Hub）下载模型配置。

以下是可能的原因和解决方案：

---

### **原因分析**
1. **`model_conf` 参数未显式提供**:
   - 在 `build_model` 方法中，代码会检查 `kwargs` 是否包含 `model_conf`：
     ```python
     if "model_conf" not in kwargs:
         logging.info("model_conf is not provided in kwargs, will download from modelscope(ms)")
         kwargs = download_model(**kwargs)
     ```
   - 如果未提供 `model_conf`，则会调用 `download_model` 方法尝试从远程下载模型配置。

2. **模型路径仅指定了权重文件**:
   - 你传入的 `model`, `vad_model`, `punc_model`, 和 `spk_model` 参数可能仅指定了模型权重文件路径（如 `model.pt`），而没有提供模型的配置文件路径（如 `model.yaml` 或 `model_conf.json`）。

3. **默认行为**:
   - 如果没有显式提供 `model_conf`，代码会假定需要从远程下载配置文件，即使你已经提供了本地模型路径。

---

### **解决方案**
#### 1. **显式提供 `model_conf` 参数**
   - 确保在 `config` 中为每个模型路径提供对应的配置文件路径，并通过 `model_conf` 参数传入。例如：
     ```python
     model = AutoModel(
         model=config["paraformer_path"],
         vad_model=config["vad_model_path"],
         punc_model=config["punc_model_path"],
         spk_model=config["spk_model_path"],
         disable_update=True,
         model_conf=config["paraformer_conf_path"],  # 显式提供模型配置文件路径
         vad_model_conf=config["vad_model_conf_path"],  # VAD 模型配置文件
         punc_model_conf=config["punc_model_conf_path"],  # 标点模型配置文件
         spk_model_conf=config["spk_model_conf_path"],  # 说话人分离模型配置文件
     )
     ```

#### 2. **检查 `download_model` 的默认行为**
   - 如果你不希望从远程下载模型配置，可以在 `config` 中确保所有必要的文件（如权重文件和配置文件）都已本地化，并避免依赖 `download_model`。

#### 3. **修改 `build_model` 的逻辑（可选）**
   - 如果你有权限修改代码，可以在 `build_model` 方法中添加逻辑，优先检查本地路径是否包含配置文件，而不是直接尝试下载：
     ```python
     if "model_conf" not in kwargs:
         local_conf_path = os.path.join(os.path.dirname(kwargs["model"]), "model_conf.yaml")
         if os.path.exists(local_conf_path):
             kwargs["model_conf"] = local_conf_path
         else:
             logging.info("model_conf is not provided in kwargs, will download from modelscope(ms)")
             kwargs = download_model(**kwargs)
     ```

---

### **总结**
你需要显式提供 `model_conf` 参数，或者确保模型路径中包含配置文件。如果配置文件已经存在但仍提示下载，可以检查 `build_model` 方法的逻辑，确保优先使用本地文件而不是默认下载行为。

---

